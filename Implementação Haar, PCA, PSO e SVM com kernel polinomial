#implementação com kernel polinomial
!pip install PyWavelets pyswarms

import numpy as np
import pandas as pd
import pywt
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Geração dos padrões
def generate_all_patterns(length=64, sigma_range=(0, 5)):
    sigma = np.random.uniform(*sigma_range)
    N_t = np.random.normal(0, sigma, length)
    start_i, start_j = sorted(np.random.choice(np.arange(10, length - 10), 2, replace=False))

    patterns = {
        'u.s. + i.t.': np.concatenate([np.zeros(start_i), np.ones(length-start_i) * np.random.uniform(1.75, 2.5)*sigma]) + np.concatenate([np.zeros(start_j), np.arange(length-start_j)*np.random.uniform(0.1, 0.2)*sigma]),
        'd.s. + d.t.': np.concatenate([np.zeros(start_i), np.ones(length-start_i) * np.random.uniform(-2.5, -1.75)*sigma]) + np.concatenate([np.zeros(start_j), np.arange(length-start_j)*np.random.uniform(-0.2, -0.1)*sigma]),
        'i.t. + c.p.': np.concatenate([np.zeros(start_i), np.arange(length-start_i)*np.random.uniform(0.1, 0.2)*sigma]) + np.sin(np.linspace(0, 2*np.pi*length/0.8, length))*np.random.uniform(2,2.5)*sigma,
        'd.t. + c.p.': np.concatenate([np.zeros(start_i), np.arange(length-start_i)*np.random.uniform(-0.2, -0.1)*sigma]) + np.sin(np.linspace(0, 2*np.pi*length/0.8, length))*np.random.uniform(2,2.5)*sigma,
        'u.s. + c.p.': np.concatenate([np.zeros(start_i), np.ones(length-start_i)*np.random.uniform(1.75, 2.5)*sigma]) + np.sin(np.linspace(0, 2*np.pi*length/0.8, length))*np.random.uniform(2,2.5)*sigma,
        'd.s. + c.p.': np.concatenate([np.zeros(start_i), np.ones(length-start_i)*np.random.uniform(-2.5, -1.75)*sigma]) + np.sin(np.linspace(0, 2*np.pi*length/0.8, length))*np.random.uniform(2,2.5)*sigma,
        'u.s. + s.p.': np.concatenate([np.zeros(start_i), np.ones(length-start_i)*np.random.uniform(1.75, 2.5)*sigma]) + np.array([(-1)**i for i in range(length)])*np.random.uniform(1.5, 3)*sigma,
        'd.s. + s.p.': np.concatenate([np.zeros(start_i), np.ones(length-start_i)*np.random.uniform(-2.5, -1.75)*sigma]) + np.array([(-1)**i for i in range(length)])*np.random.uniform(1.5, 3)*sigma,
        'normal': N_t
    }

    for key in patterns:
        if key != 'normal':
            patterns[key] += N_t

    return patterns

# 2. Decomposição Haar Wavelet
def haar_wavelet_decompose(signal, level=6):
    coeffs = pywt.wavedec(signal, 'haar', level=level)
    return coeffs[0], coeffs[1:]

# 3. Particle Swarm Optimization (PSO)
class Particle:
    def __init__(self, bounds):
        self.position = np.array([np.random.uniform(low, high) for low, high in bounds])
        self.velocity = np.random.uniform(-4, 4, len(bounds))
        self.best_pos = np.copy(self.position)
        self.best_err = float('inf')
        self.err = float('inf')

def pso(objective_func, bounds, num_particles, maxiter):
    swarm = [Particle(bounds) for _ in range(num_particles)]
    global_best_pos = np.zeros(len(bounds))
    global_best_err = float('inf')

    for iteration in range(maxiter):
        for particle in swarm:
            particle.err = objective_func(particle.position)

            if particle.err < particle.best_err:
                particle.best_err = particle.err
                particle.best_pos = particle.position.copy()

            if particle.err < global_best_err:
                global_best_err = particle.err
                global_best_pos = particle.position.copy()

        for particle in swarm:
            wp = np.random.uniform(0.2, 0.8)
            r1, r2 = np.random.rand(2)
            cognitive = r1 * (particle.best_pos - particle.position)
            social = r2 * (global_best_pos - particle.position)

            particle.velocity = wp * particle.velocity + cognitive + social
            particle.velocity = np.clip(particle.velocity, -4, 4)
            particle.position += particle.velocity
            particle.position = np.clip(particle.position, [b[0] for b in bounds], [b[1] for b in bounds])

    return global_best_pos, global_best_err

# 4. Geração do Dataset
sample_config = {
    'u.s. + i.t.': 30, 'd.s. + d.t.': 30, 'i.t. + c.p.': 60, 'd.t. + c.p.': 60,
    'u.s. + c.p.': 80, 'd.s. + c.p.': 80, 'u.s. + s.p.': 64, 'd.s. + s.p.': 64, 'normal': 64,
    'u.s.': 64, 'd.s.': 64, 'i.t.': 64, 'd.t.': 64, 'c.p.': 64, 's.p.': 64
}

length = 64

def generate_simple_pattern(label, sigma_range=(0, 5)):
    sigma = np.random.uniform(*sigma_range)
    N_t = np.random.normal(0, sigma, length)
    if label == 'u.s.':
        pattern = np.concatenate([np.zeros(20), np.ones(length - 20) * np.random.uniform(1.75, 2.5) * sigma])
    elif label == 'd.s.':
        pattern = np.concatenate([np.zeros(20), np.ones(length - 20) * np.random.uniform(-2.5, -1.75) * sigma])
    elif label == 'i.t.':
        pattern = np.concatenate([np.zeros(20), np.arange(length - 20) * np.random.uniform(0.1, 0.2) * sigma])
    elif label == 'd.t.':
        pattern = np.concatenate([np.zeros(20), np.arange(length - 20) * np.random.uniform(-0.2, -0.1) * sigma])
    elif label == 'c.p.':
        pattern = np.sin(np.linspace(0, 2 * np.pi * length / 0.8, length)) * np.random.uniform(2, 2.5) * sigma
    elif label == 's.p.':
        pattern = np.array([(-1)**i for i in range(length)]) * np.random.uniform(1.5, 3) * sigma
    else:
        pattern = N_t
    return pattern + N_t

# Dataset final
X_data, y_data = [], []
for label, n_samples in sample_config.items():
    for _ in range(n_samples):
        if label in ['u.s.', 'd.s.', 'i.t.', 'd.t.', 'c.p.', 's.p.', 'normal']:
            X_data.append(generate_simple_pattern(label))
        else:
            X_data.append(generate_all_patterns()[label])
        y_data.append(label)

X_data = np.array(X_data)
y_data = np.array(y_data)

# Treinamento/Teste
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)

# 5. PSO + SVM Polinomial
def svm_fitness_poly(params):
    C, degree = params
    degree = int(round(degree))
    try:
        clf = svm.SVC(C=C, kernel='poly', degree=degree, coef0=1)
        clf.fit(X_train, y_train)
        preds = clf.predict(X_train)
        return 1 - accuracy_score(y_train, preds)
    except Exception:
        return 1

bounds_poly = [(1e-2, 1e3), (2, 5)]
best_pos_poly, best_err_poly = pso(svm_fitness_poly, bounds_poly, num_particles=30, maxiter=30)
C_opt_poly, degree_opt_poly = best_pos_poly
degree_opt_poly = int(round(degree_opt_poly))

model_poly = svm.SVC(kernel='poly', C=C_opt_poly, degree=degree_opt_poly, coef0=1)
model_poly.fit(X_train, y_train)
y_pred_poly = model_poly.predict(X_test)

# Avaliação
def evaluate_model(model, X_test, y_test, y_pred, best_params):
    print("\n Parâmetros otimizados pelo PSO:")
    for k, v in best_params.items():
        print(f"{k} = {v:.4f}" if isinstance(v, float) else f"{k} = {v}")

    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n Acurácia do modelo: {accuracy * 100:.2f}%")

    labels = np.unique(y_test)
    cm = confusion_matrix(y_test, y_pred, labels=labels)
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    print("\n Matriz de Confusão (% por classe real):")
    plt.figure(figsize=(12, 8))
    sns.heatmap(cm_percent, annot=True, fmt=".1f", cmap="Blues",
                xticklabels=labels, yticklabels=labels)
    plt.ylabel('Classe real')
    plt.xlabel('Classe predita')
    plt.title('Matriz de Confusão (%)')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    print("\n Relatório detalhado da classificação:")
    print(classification_report(y_test, y_pred))

# Rodar avaliação final
best_params_poly = {'C': C_opt_poly, 'degree': degree_opt_poly}
evaluate_model(model_poly, X_test, y_test, y_pred_poly, best_params_poly)
